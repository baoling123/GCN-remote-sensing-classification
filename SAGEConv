class HybridResNet18GNN(torch.nn.Module):
    def __init__(self, in_channels, num_classes, hidden_channels=64):
        super(HybridResNet18GNN, self).__init__()
        self.initial_conv = SAGEConv(in_channels, hidden_channels)
        self.initial_bn = BatchNorm1d(hidden_channels)
        self.initial_relu = ReLU(inplace=False) 

        self.block1 = HybridResNetBlock(hidden_channels, hidden_channels)
        self.block2 = HybridResNetBlock(hidden_channels, hidden_channels)
        self.block3 = HybridResNetBlock(hidden_channels, hidden_channels * 2)
        self.block4 = HybridResNetBlock(hidden_channels * 2, hidden_channels * 2)
        self.block5 = HybridResNetBlock(hidden_channels * 2, hidden_channels * 4)
        self.block6 = HybridResNetBlock(hidden_channels * 4, hidden_channels * 4)
        self.block7 = HybridResNetBlock(hidden_channels * 4, hidden_channels * 8)
        self.block8 = HybridResNetBlock(hidden_channels * 8, hidden_channels * 8)

        self.fc = Linear(hidden_channels * 8, num_classes)

    def forward(self, x, edge_index):
        x = self.initial_conv(x, edge_index)  
        x = self.initial_bn(x)
        x = self.initial_relu(x)

        x = self.block1(x, edge_index)
        x = self.block2(x, edge_index)
        x = self.block3(x, edge_index)
        x = self.block4(x, edge_index)
        x = self.block5(x, edge_index)
        x = self.block6(x, edge_index)
        x = self.block7(x, edge_index)
        x = self.block8(x, edge_index)

     
        x = self.fc(x)
        return F.log_softmax(x, dim=1)  
